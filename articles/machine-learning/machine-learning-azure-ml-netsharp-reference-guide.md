<properties 
    pageTitle="Net# ニューラル ネットワーク仕様言語について | Microsoft Azure" 
    description="構文の Net # ニューラル ネットワーク仕様言語と、Net を使用して Microsoft Azure ML でカスタム ニューラル ネットワーク モデルを作成する方法の例#" 
    services="machine-learning" 
    documentationCenter="" 
    authors="jeannt" 
    manager="paulettm" 
    editor="cgronlun"/>

<tags 
    ms.service="machine-learning" 
    ms.workload="data-services" 
    ms.tgt_pltfrm="na" 
    ms.devlang="na" 
    ms.topic="article" 
    ms.date="10/06/2015" 
    ms.author="jeannt"/>



# Azure Machine Learning のための Net# ニューラル ネットワーク仕様言語について

## 概要
Net# は、Microsoft が開発した、Microsoft Azure Machine Learning のニューラル ネットワーク モジュールのためのニューラル ネットワーク アーキテクチャを定義するために使用される言語です。 この記事では、次のことについて説明します。  

-   ニューラル ネットワークに関連する基本的な概念
-   ニューラル ネットワークの前提条件と主なコンポーネントの定義方法
-   Net# 仕様言語の構文とキーワード
-   Net# を使用したカスタム ニューラル ネットワークの例# 
    
[AZURE.INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]  

## ニューラル ネットワークの基本
ニューラル ネットワーク構造から成る ***ノード*** で編成される ***レイヤー***, 、加重計算される ***接続*** (または ***エッジ***) のノード間でします。 結合には方向があり、および各接続には、 ***ソース*** ノードおよび ***宛先*** ノードです。  

各 ***トレーニング可能層*** (隠れ層または出力層) が 1 つまたは複数 ***結合バンドル***します。 結合バンドルは、ソース層と、そのソース層からの結合の仕様で構成されています。 指定のバンドル内のすべての接続が同じ ***ソース層*** と同じ ***宛て先層***します。 Net# では、結合バンドルはバンドルの宛先層に属すると見なされます。  
 
Net# は、隠れ層や出力への入力のマッピング方法をカスタマイズできる、さまざまな種類の結合バンドルをサポートします。   

既定値または標準のバンドルは、 **フル バンドル**, 、ソース層の各ノードが宛て先層内の各ノードに接続されているのです。  

これに加えて、Net# は以下の 4 種類の詳細な結合バンドルをサポートします。  

-   **フィルター バンドル**します。 ユーザーは、ソース層ノードと宛先層ノードの場所を使用して述語を定義できます。 述語が True の場合は常にノードが結合されます。
-   **畳み込みバンドル**します。 ユーザーはソース層の少数の隣接ノードを定義できます。 宛先層の各ノードは、ソース層の隣接ノードの 1 つに結合します。
-   **プーリング バンドル** と **応答正規化バンドル**します。 これは、ユーザーがソース層の少数の隣接ノードを定義するという点で畳み込みバンドルと似ています。 違いは、これらのバンドルのエッジの重みがトレーニング可能でないことです。 代わりに、定義済みの関数がソース ノード値に適用され、宛先ノードの値を判断します。  

ニューラル ネットワークの構造を定義するために Net# を使用することで、ディープ ニューラル ネットワークや任意次元の畳み込みなどの複雑な構造を定義することが可能になり、これが画像、音声、映像などのデータを使用する学習に役立つことが知られています。  

## サポートされるカスタマイズ
Azure Machine Learning で作成するニューラル ネットワーク モデルのアーキテクチャを使用すると、Net# で高度なカスタマイズができます。 そのための方法は次のとおりです。  

-   隠れ層を作成し、各層のノード数を管理する。
-   層間の結合方法を指定する。
-   畳み込みや重み共有バンドルなどの特殊な結合性の構造を定義する。
-   複数のアクティブ化関数を指定する。  

仕様言語の構文の詳細については、「 [構造の仕様](#Structure-specifications)します。  
 
一般的な機械学習、単純または複雑なタスク用のニューラル ネットワークを定義する例については、次を参照してください。 [例](#Examples-of-Net#-usage)します。  

## 一般的な要件
-   厳密に 1 つの出力層、少なくとも 1 つの入力層、およびゼロかそれ以上の隠れ層がなければなりません。 
-   各層には固定された数のノードが含まれ、これらのノードは概念的には任意の次元を長方形配列に格納したものです。 
-   入力層には関連付けられているトレーニング済みのパラメーターはなく、インスタンス データがネットワークに入る点を表します。 
-   トレーニング可能な層 (隠れ層と出力層) には、重みおよびバイアスと呼ばれるトレーニング済みパラメーターが関連付けられています。 
-   ソース ノードと宛先ノードは別々の層になければなりません。 
-   結合は非循環でなければなりません。つまり、最初のソース ノードに戻る環状の結合が存在することはできません。
-   結合バンドルのソース層を出力層にすることはできません。  

## 構造の仕様
ニューラル ネットワーク構造仕様は 3 つのセクションで構成されます。 **定数の宣言**, 、 **レイヤーの宣言**, 、 **結合の宣言**します。 省略可能な **共有の宣言** セクションです。 セクションは任意の順序で指定できます。  

## 定数宣言 
定数宣言は省略可能です。 これは、ニューラル ネットワーク定義内で使用する値を定義する手段を提供します。 宣言のステートメントは、識別子とそれに続く等号、および値の式で構成されています。   

たとえば、次のステートメントは定数を定義 **x**:  


    Const X = 28;  

2 つ以上の定数を同時に定義するには、識別子の名前と値を中かっこで囲み、セミコロンで区切ります。 次に例を示します。  

    Const { X = 28; Y = 4; }  

それぞれの代入式の右側には、整数、実数、ブール値 (True または False)、あるいは数式を使用することができます。 次に例を示します。  

    Const { X = 17 * 2; Y = true; }  

## 層の宣言
層の宣言が必要です。 これは、サイズと、接続のバンドルと属性を含む、層のソースを定義します。 宣言ステートメントは、層の名前 (input、hidden または output) で始まり、それに層の次元 (正の整数のタプル) が続きます。 次に例を示します。  

    input Data auto;
    hidden Hidden[5,20] from Data all;
    output Result[2] from Hidden all;  

-   次元を乗算すると、層内のノード数になります。 この例では、層内にあるが 100 個のノードであることを意味する 2 つのディメンション [5, 20] があります。
-   層は任意の順序で宣言することができます。ただし例外的に、2 つ以上の入力層を定義する場合は、定義の順序が入力データ内の機能の順序に一致する必要があります。  


層内のノードの数が自動的に決定することを指定するには、使用、 **自動** キーワードです。  **自動** キーワードによっては、レイヤーのさまざまな効果には。  

-   入力層の宣言では、ノード数は入力データの機能の数になります。
-   隠れ層の宣言では、ノードの数がのパラメーターの値で指定されている番号 **隠しノード数**します。 
-   出力層の宣言におけるノード数は、2 クラスの分類の場合は 2、回帰の場合は 1、複数クラスの分類では出力ノードの数と同数になります。   

たとえば、次のようにネットワークを定義すると、すべての層のサイズを自動的に決定できます。  

    input Data auto;
    hidden Hidden auto from Data all;
    output Result auto from Hidden all;  


トレーニング可能層 (隠れ層または出力層) の層の宣言は出力関数 (アクティブ化関数とも呼ばれます)、既定ではオプションで含める **シグモイド** 分類モデルと **線形** 回帰モデルです。 (既定を使用する場合でも、わかりやすくするのであれば、アクティブ化関数を明示的に指定できます。)

次の出力関数がサポートされています。  

-   sigmoid
-   linear
-   softmax
-   rlinear
-   square
-   sqrt
-   srlinear
-   abs
-   tanh 
-   brlinear  

たとえば、次の宣言では、 **softmax** 関数。  

    output Result [100] softmax from Hidden all;  

## 結合の宣言
トレーニング可能な層の定義が終了したら、その直後に、定義した層間の結合を宣言する必要があります。 結合バンドルの宣言は、キーワードで始まる **から**, バンドルのソース層、および作成する結合バンドルの種類の名前と、その後です。   

現在、以下の 5 種類の結合バンドルがサポートされています。  

-   **完全** バンドル キーワードを使用して表されます **すべて。**
-   **フィルター処理された** バンドル キーワードを使用して表されます **、**, と続く述語式。
-   **畳み込み** バンドル キーワードを使用して表されます **convolve**, と続く畳み込み属性。
-   **プーリング** バンドル キーワードで表されます **プールの最大** または **という意味ではプール。**
-   **応答正規化** バンドル キーワードを使用して表されます **応答 norm。**      

## フル バンドル  

ソース層の各ノードから宛先層の各ノードへの結合を含む、全結合バンドルです。 これが既定のネットワーク結合のタイプです。  

## フィルター バンドル
フィルター処理される結合バンドル仕様には、C# lambda 式によく似た構文で表された述語が含まれています。 以下に、2 つのフィルター バンドルの定義例を示します。  

    input Pixels [10, 20];
    hidden ByRow[10, 12] from Pixels where (s,d) => s[0] == d[0];
    hidden ByCol[5, 20] from Pixels where (s,d) => abs(s[1] - d[1]) <= 1;  

-   述語で _ByRow_, 、**s** 入力層のノードの長方形配列へのインデックスを表すパラメーター _ピクセル_, 、および **d** 隠れ層のノードの配列へのインデックスを表すパラメーター _ByRow_します。 両方の型 **s** と **d** は、長さ 2 の整数のタプルです。 概念的には、 **s** を持つ整数のすべての組み合わせにわたる範囲 _0 < = s [0] < 10_ と _0 < = s[1] < 20_, 、および **d** で、整数のすべてのペアにわたる範囲 _0 < = d [0] < 10_ と _0 < = d[1] < 12_です。 
-   述語式の右側に条件があります。 値ごとに、この例では **s** と **d** 宛て先層ノード、ソース層のノード間にエッジがあることなど、条件が True の場合。 したがって、このフィルター式では、バンドルにはで定義済みのノードからの接続が含まれていることを示します **s** によって定義済みのノードに **d** 常に s [0] が d [0]。  

フィルター処理されたバンドルにはオプションで重みのセットを指定できます。 値、 **重み** 属性が浮動小数点、長さがバンドルで定義されている接続の数が一致する値のタプルにする必要があります。 既定では、重みは無作為に生成されます。  

重みの値は、宛先ノードのインデックスでグループ化されます。 つまり最初の宛先ノードが k 個のソース ノードに接続されている場合は、最初の _K_ の要素、 **重み** 組は、ソースのインデックス順では、最初の宛先ノードの重み。 残りの宛先ノードについても同様に適用されます。  

重みは、定数値として直接指定することができます。 たとえば、以前の重みを知っている場合は、次の構文で定数値として重みを指定できます。

    const Weights_1 = [0.0188045055, 0.130500451, ...]


## 畳み込みバンドル
トレーニング データが同質な構造を持つ場合、データの高レベル機能を学習するためには、一般に、畳み込み結合が使用されます。 たとえば、画像、音声、映像のデータでは、空間的または時間的なディメンショナリティがかなり均一です。  

畳み込みバンドルを使用して四角形 **カーネル** を次元間をスライドします。 基本的には、各カーネルと呼ばれる、局所的に適用される重みのセットを定義 **のカーネル アプリケーション**します。 呼ばれるソース層内のノードに対応するそれぞれのカーネル アプリケーション、 **中央ノード**します。 カーネルの重みは多くの結合で共有されます。 畳み込みバンドルでは、各カーネルは四角形ですべてのカーネル アプリケーションは同一サイズです。  

畳み込みバンドルは、以下の属性をサポートします。

**InputShape** この畳み込みバンドルのため、ソース層のディメンショナリティを定義します。 値は正の整数のタプルとする必要があります。 整数を乗算した数値はソース層内のノード数と一致している必要がありますが、それ以外はソース層で宣言されたディメンショナリティと一致する必要はありません。 このタプルの長さ、 **アリティ** 畳み込みバンドルのための値。 (通常、アリティとは、関数が取ることができる引数またはオペランドの数を指します)。  

形状と、カーネルの場所を定義するには、属性を使用して **KernelShape**, 、**Stride**, 、**Padding**, 、**LowerPad**, 、および **UpperPad**:   

-   **KernelShape**: (必須) 畳み込みバンドルのカーネルごとにディメンショナリティを定義します。 値は、長さ、バンドルのアリティに相当する正の整数のタプルにする必要があります。 このタプルの各要素は、対応する要素の値である必要があります **InputShape**します。 
-   **Stride**: (省略可能) に、中央ノード間の距離は、畳み込み (ディメンションごとに 1 つのステップ サイズ) のスライディング ステップ サイズを定義します。 値は、長さ、バンドルのアリティに相当する正の整数のタプルにする必要があります。 このタプルの各要素は、対応する要素の値である必要があります **KernelShape**します。 既定値は、すべての要素が 1 に等しいタプルです。 
-   **共有**: (オプション) 畳み込みの各次元で共有する重みを定義します。 値としては、単一のブール値、またはバンドルのアリティを長さとするブール値のタプルが可能です。 単一のブール値は、すべての要素が指定された値に等しい、正しい長さのタプルに拡張されます。 既定値は、すべての値が True であるタプルです。 
-   **MapCount**: (オプション) 畳み込みバンドルの機能の数がマップを定義します。 値としては、単一の正の整数、またはバンドルのアリティを長さとする正の整数のタプルが可能です。 単一の整数値は、最初の要素が指定された値に等しく、残りのすべての要素が 1 に等しい、正しい長さのタプルに拡張されます。 既定値は 1 です。 機能マップの合計数は、タプルの要素を乗算した数です。 この合計数を要素ごとに因数分解することにより、宛先ノードでの機能マップ値のグループ化の方法が決定されます。 
-   **重み**: (オプション) バンドルの最初の重みを定義します。 値は、下記の定義のように、カーネル数にカーネルあたりの重みの数を乗算した結果を長さとする、浮動小数点値のタプルです。 既定の重みは無作為に生成されます。  

埋め込みを管理するプロパティ セットは 2 つあり、これらは相互に排他的です。

-   **Padding**: (省略可能) を指定するかどうかを使用して、入力を埋め込む必要がある、 **既定の埋め込みスキーム**します。 値は、単一のブール値、またはバンドルのアリティを長さとするブール値のタプルにすることができます。 単一のブール値は、すべての要素が指定された値に等しい、正しい長さのタプルに拡張されます。 次元の値が True の場合、ソースはその次元に、ゼロ値のセルを使用して論理的に埋め込まれます。これは、その次元の最初と最後のカーネルの中央ノードがソース層のその次元の最初と最後のノードであるような、追加のカーネル アプリケーションをサポートするためです。 したがって、各次元の「ダミー」のノードの数が自動的に決定、ちょうど収まるように _(InputShape [d] - 1)/Stride [d] + 1_ 埋め込まれるソース レイヤーにカーネルです。 次元の値が False の場合は、各側で残されるノードの数が等しくなるように (誤差は最大で 1)、カーネルが定義されます。 この属性の既定値は、すべての要素が False に等しいタプルです。
-   **UpperPad** と **LowerPad**: (省略可能) を指定してさらに制御を使用する埋め込みの量。 **重要:** 場合にのみこれらの属性があることができます、 **Padding** 上記のプロパティは ***いない*** 定義します。 値は、整数値のタプルで、長さがバンドルのアリティである必要があります。 これらの属性が指定されると、入力層の各次元の最上部と最下部に、"ダミー" のノードが追加されます。 各次元の下限と上限の端に追加されるノードの数はによって決まります **LowerPad**[i] と **UpperPad**[i] それぞれします。 カーネルが "ダミー" ではない "実際" のノードのみに対応するようにするには、以下の条件を満たす必要があります。
    -   各コンポーネントの **LowerPad** する必要があります KernelShape [d] よりも厳密に小さい/2。 
    -   各要素の **UpperPad** する必要があります、kernelshape [d]/2。 
    -   これらの属性の既定値は、すべての要素が 0 に等しいタプルです。 

設定 **Padding** = true の場合は、"center"のカーネルの"real"内の入力を保持するために必要なの余白をできます。 これにより、出力サイズを計算するための演算が若干変更されます。 一般に、出力サイズ _D_ として計算されます _D = (I、K)/S + 1_, ここで、 _は_ 入力サイズは、 _K_ カーネルのサイズは、 _S_ 、stride と _/_ 整数除算 (ゼロに丸める round) は、です。 = UpperPad を設定する [1, 1] 入力サイズ _は_ は事実上 29 になり _D = (29-5)/2 + 1 = 13_します。 ただし、 **Padding** = true、基本的に _は_ によって _K-1_ため _D = ((28 + 4) - 5)/27 = 2 + 1/2 + 1 = 13 + 1 = 14_します。 値を指定して **UpperPad** と **LowerPad** だけ設定ときよりも埋め込みをより細かく制御を取得する **Padding** = true です。

畳み込みネットワークとそのアプリケーションの詳細については、以下の記事を参照してください。  

-   [http://deeplearning.net/tutorial/lenet.html ](http://deeplearning.net/tutorial/lenet.html )
-   [http://research.microsoft.com/pubs/68920/icdar03.pdf](http://research.microsoft.com/pubs/68920/icdar03.pdf) 
-   [http://people.csail.mit.edu/jvb/papers/cnn_tutorial.pdf](http://people.csail.mit.edu/jvb/papers/cnn_tutorial.pdf)  

## プーリング バンドル
A **プーリング バンドル** 畳み込みの結合性と同様のジオメトリの適用をソース ノード値に事前定義された関数を使用して、宛先ノード値を派生することができます。 このため、プーリング バンドルにはトレーニング可能な状態 (重みまたはバイアス) はありません。 プーリング バンドルのサポートを除くすべての畳み込み属性 **共有**, 、**MapCount**, 、および **重み**します。  

一般に、隣接するプーリング ユニットでまとめられたカーネルが重複することはありません。 各次元で Stride[d] が KernelShape[d] に等しい場合、取得される層は、畳み込みニューラル ネットワークで一般に採用される、従来型のローカル プーリング層です。 各宛先ノードで、ソース層のカーネルのアクティビティの最大値または平均値を計算します。  

以下にプーリング バンドルの例を示します。 

    hidden P1 [5, 12, 12]
      from C1 max pool {
        InputShape  = [ 5, 24, 24];
        KernelShape = [ 1,  2,  2];
        Stride      = [ 1,  2,  2];
      }  

-   バンドルのアリティは 3 (タプルの長さ **InputShape**, 、**KernelShape**, 、および **Stride**)。 
-   ソース層のノードの数が _5 * * 24 24 = 2880_します。 
-   これは、ためにの従来型のローカル プーリング層 **KernelShape** と **Stride** が等しい。 
-   宛て先層のノードの数が _5 * 12 * 12 = 1440_します。  
    
プーリング層の詳細については、以下の記事を参照してください。  

-   [http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf) (セクション 3.4)
-   [http://cs.nyu.edu/~koray/publis/lecun-iscas-10.pdf](http://cs.nyu.edu/~koray/publis/lecun-iscas-10.pdf) 
-   [http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf](http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf)
    
## 応答正規化バンドル
**応答正規化** 、Geoffrey Hinton で初めて導入された、ローカル正規化スキームは、ホワイト ペーパーでのほか、 [ディープ畳み込みニューラル ネットワークで ﬁ Classiﬁcation](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf)します。 応答正規化は、ニューラル ネットの一般化を支援するために使用します。 あるニューロンが高度な活性レベルで発火しているときに、ローカルな応答正規化層によって周囲のニューロンの活性レベルを抑えます。 3 つのパラメーターを使用して、これは、(***α***, 、***β***, 、および ***k***) と、畳み込み構造体 (または隣接する形状) です。 宛て先層のすべてのニューロン ***y*** ニューロンに対応する ***x*** で、ソース層です。 活性レベル ***y*** 次の式によって指定された場所 ***f*** 、ニューロンの活性レベルと ***Nx*** カーネル (または隣接するニューロンを含むセット ***x***) 次の畳み込み構造体によって定義された。  

![][1]  

応答正規化バンドルを除くすべての畳み込み属性をサポートする **共有**, 、**MapCount**, 、および **重み**します。  
 
-   カーネルと同じマップにニューロンを含んでいるかどうかは ***x***, 、その正規化スキームと呼ばれます **正規化をマップ同じ**します。 同一マップ正規化を第 1 座標を定義する **InputShape** 値 1 を持つ必要があります。
-   カーネルと同じ空間位置にニューロンを含んでいるかどうかは ***x***, が、そのニューロンが他のマップ内で、その正規化スキームが呼び出さ **で正規化をマップ**します。 このタイプの応答正規化では、側方抑制の形式が実装されます。これは、実際のニューロンで検出されたタイプから着想されたもので、異なるマップで計算されたニューロン出力間で大きな活性レベルに対する競合が発生します。 マップ間での正規化を定義するには、第 1 座標が 1 を超える整数で、マップの数以下である必要があります。残りの座標は値 1 を持つ必要があります。  

応答正規化バンドルは、事前定義関数をソース ノード値に適用して宛先ノード値を決定するため、トレーニング可能状態 (重みまたはバイアス) はありません。   

**警告**: 宛て先層のノードは、カーネルの中央ノードであるニューロンに対応します。 たとえば、KernelShape [d] が、奇数 _KernelShape [d]/2_ が中央カーネル ノードに対応しています。 場合 _KernelShape [d]_ が偶数の中央ノードは _KernelShape [d]/2-1_します。 そのため場合、 **Padding**[d] が False の場合、最初と最後の _KernelShape [d]/2_ ノードでは、宛て先層には、対応するノードはありません。 このような状況を避けるためには、次のように定義します。 **Padding** として [true の場合、true,…, true] です。  

応答正規化バンドルは、先に説明した 4 つの属性に加え、以下の属性もサポートします。  

-   **アルファ**: (必須) に対応する浮動小数点値を指定 ***α*** 前の公式です。 
-   **ベータ版**: (必須) に対応する浮動小数点値を指定 ***β*** 前の公式です。 
-   **オフセット**: (省略可能) に対応する浮動小数点値を指定します ***k*** 前の公式です。 既定値は 1 です。  

次に、これらの属性を使用して応答正規化バンドルを定義する例を示します。  

    hidden RN1 [5, 10, 10]
      from P1 response norm {
        InputShape  = [ 5, 12, 12];
        KernelShape = [ 1,  3,  3];
        Alpha = 0.001;
        Beta = 0.75;
      }  

-   ソース層には、5 つのマップが含まれ、12 x 12 の各次元に、合計 1440 ノードがあります。 
-   値 **KernelShape** ネットワーク コンピューターが、3 × 3 の四角形が同じマップ正規化層であることを示します。 
-   既定値の **Padding** false で、ため、宛て先層は次元あたり 10 ノードのみは含まれています。 ソース層のどのノードにも宛先層の 1 つのノードが対応するようにするには、Padding = [true, true, true]; を追加し、RN1 のサイズを [5, 12, 12] に変更します。  

## 共有の宣言 
Net# では、オプションで、共有の重みを持つ複数のバンドルの定義をサポートしています。 構造が同じであれば、任意の 2 つのバンドルの重みを共有することができます。 次の構文には共有の重みのバンドルを定義します。  

    share-declaration:
        share    {    layer-list    }
        share    {    bundle-list    }
       share    {    bias-list    }
    
    layer-list:
        layer-name    ,    layer-name
        layer-list    ,    layer-name
    
    bundle-list:
       bundle-spec    ,    bundle-spec
        bundle-list    ,    bundle-spec
    
    bundle-spec:
       layer-name    =>     layer-name
    
    bias-list:
        bias-spec    ,    bias-spec
        bias-list    ,    bias-spec
    
    bias-spec:
        1    =>    layer-name
    
    layer-name:
        identifier  

たとえば、次の share 宣言では、層の名前を指定し、重みとバイアスの両方を共有するように指示しています。  

    Const {
      InputSize = 37;
      HiddenSize = 50;
    }
    input {
      Data1 [InputSize];
      Data2 [InputSize];
    }
    hidden {
      H1 [HiddenSize] from Data1 all;
      H2 [HiddenSize] from Data2 all;
    }
    output Result [2] {
      from H1 all;
      from H2 all;
    }
    share { H1, H2 } // share both weights and biases  

-   入力機能は、2 つの同じサイズの入力層に分割されます。 
-   次に、隠れ層が 2 つの入力層の高い方のレベルの機能を計算します。 
-   共有宣言を指定する _H1_ と _H2_ それぞれの入力から同じ方法で計算する必要があります。  
 
または、次のように、これを 2 つの個別の共有宣言で指定することもできます。  

    share { Data1 => H1, Data2 => H2 } // share weights  

<!-- -->

    share { 1 => H1, 1 => H2 } // share biases  

この簡易方法は、層に単一バンドルが含まれている場合のみ使用できます。 一般に、共有が可能なのは、該当する構造が同一の場合に限られます。つまり、サイズ、畳み込みの構造などが同一である場合です。  

## Net# の使用例
このセクションでは、隠れ層の追加、隠れ層の他の層との対話方法の定義、および畳み込みネットワークの構築のための Net# の使用方法の例を示します。   

### シンプルなカスタム ニューラル ネットワークの定義: "Hello World" の例
単一の隠れ層を持つニューラル ネットワーク モデルの作成方法をこのシンプルな例で示します。  

    input Data auto;
    hidden H [200] from Data all;
    output Out [10] sigmoid from H all;  

例では、次のようにいくつかの基本的なコマンドを示しています。  

-   最初の行は、入力層を定義します。 (名前付き _データ_)。 使用すると、  **自動** キーワード、ニューラル ネットワークでは、入力例のすべての特徴列に自動的が含まれています。 
-   2 行目で隠れ層が作成されます。 名前 _H_ が 200 個のノードを非表示の層に割り当てられます。 この層は入力層に完全結合されます。
-   3 行目は、出力層を定義します。 (名前付き _O_)、10 個の出力ノードが含まれています。 ニューラル ネットワークが分類に使用される場合、クラスあたり 1 つの出力ノードがあります。 キーワード **シグモイド** 出力関数は、出力層に適用されていることを示します。   

### 複数の隠れ層の定義: コンピューター ビジョンの例
次に、複数のカスタム隠れ層を持つ、もう少し複雑なニューラル ネットワークを定義する例を示します。  

    // Define the input layers 
    input Pixels [10, 20];
    input MetaData [7];
    
    // Define the first two hidden layers, using data only from the Pixels input
    hidden ByRow [10, 12] from Pixels where (s,d) => s[0] == d[0];
    hidden ByCol [5, 20] from Pixels where (s,d) => abs(s[1] - d[1]) <= 1;
    
    // Define the third hidden layer, which uses as source the hidden layers ByRow and ByCol
    hidden Gather [100] 
    {
      from ByRow all;
      from ByCol all;
    }
    
    // Define the output layer and its sources
    output Result [10]  
    {
      from Gather all;
      from MetaData all;
    }  

このサンプルでは、以下のニューラル ネットワーク仕様言語の複数の機能を示しています。  

-   構造には、2 つの入力層 _ピクセル_ と _メタデータ_します。
-    _ピクセル_ 層は宛て先層に 2 つの結合バンドルに対するソース層 _ByRow_ と _ByCol_します。
-   レイヤー _収集_ と _結果_ は複数の接続バンドルにある宛て先層です。
-   出力層 _結果_, 、2 つの宛て先層です結合バンドル 1、2 番目の非表示 (Gather) 宛て先層として、もう一方は宛て先層として入力層 (メタデータ)。
-   隠れ層の _ByRow_ と _ByCol_, 、フィルター選択された述語式を使用して指定します。 内のノードでは正確には、 _ByRow_ にある [x, y] 内のノードに接続されている _ピクセル_ 最初の座標 x のことが最初のインデックス座標のノードに等しい。 [X, y] にある _ByCol 内のノードでは同様に、内のノードに接続されている _ピクセル_ 2 番目の座標、y をことが 2 番目のインデックス座標のノードのいずれかにします。  

### 複数クラス分類の畳み込みネットワークの定義: 数字認識の例
次のネットワークの定義は数字を認識するように設計され、ニューラル ネットワークの高度なカスタマイズ技法を表しています。  

    input Image [29, 29];
    hidden Conv1 [5, 13, 13] from Image convolve 
    {
       InputShape  = [29, 29];
       KernelShape = [ 5,  5];
       Stride      = [ 2,  2];
       MapCount    = 5;
    }
    hidden Conv2 [50, 5, 5]
    from Conv1 convolve 
    {
       InputShape  = [ 5, 13, 13];
       KernelShape = [ 1,  5,  5];
       Stride      = [ 1,  2,  2];
       Sharing     = [false, true, true];
       MapCount    = 10;
    }
    hidden Hid3 [100] from Conv2 all;
    output Digit [10] from Hid3 all;  


-   構造に 1 つの入力層 _イメージ_します。
-   キーワード **convolve** レイヤーがという名前を示す _Conv1_ と _Conv2_ が畳み込み層です。 これらの各層の宣言には、畳み込み属性の一覧が続きます。
-   Net は第 3 の隠れ層 _Hid3_, 、2 番目の隠れ層に完全に接続される _Conv2_します。
-   出力層 _桁_, 、3 番目の隠れ層のみに接続されている _Hid3_します。 キーワード **すべて** 出力層が完全に接続されていることを示す _Hid3_します。
-   畳み込みのアリティは 3 (タプルの長さ **InputShape**, 、**KernelShape**, 、**Stride**, 、および **共有**)。 
-   カーネルごとの重みの数が _1 + **KernelShape**\[0] * **KernelShape**\[1] * **KernelShape**\[2] = 1 + 1 * 5 * 5 = 26 です。または、26 * 50 = 1300_ になります。
-   それぞれの隠れ層のノードは、次のように計算できます。
    -   **NodeCount**\[0] = (5-1)/1 + 1 = 5 です。
    -   **NodeCount**\[1] = (13-5)/2 + 1 = 5 です。 
    -   **NodeCount**\[2] = (13-5)/2 + 1 = 5 です。 
-   ノードの合計数は、層の宣言済みディメンショナリティを使用して計算できます [50, 5, 5]、次のように: _**MapCount** * **NodeCount**\[0] * **NodeCount**\[1] * **NodeCount**\[2] = 10 * 5 * 5 * 5_
-    **共有**[d] が False のみ _d 0 = =_, 、カーネルの数は _**MapCount** * **NodeCount**\[0] = 10 * 5 = 50_します。 


## 謝辞

ニューラル ネットワークのアーキテクチャをカスタマイズするための Net# 言語は Microsoft の Shon Katzenberger (設計者、Machine Learning) と Alexey Kamenev (ソフトウェア エンジニア、Microsoft Research) により開発されました。 機械学習と、画像検出からテキスト分析までの用途に社内利用されています。 詳細については、次を参照してください [ニューラル ネットの Azure ML で Net # の概要。](http://blogs.technet.com/b/machinelearning/archive/2015/02/16/neural-nets-in-azure-ml-introduction-to-net.aspx)


[1]:./media/machine-learning-azure-ml-netsharp-reference-guide/formula_large.gif
 


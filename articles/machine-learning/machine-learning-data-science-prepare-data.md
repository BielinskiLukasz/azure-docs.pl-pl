<properties
    pageTitle="機械学習を強化するためのデータを準備するタスク | Microsoft Azure"
    description="データの前処理とクリーンアップを行って機械学習の準備を整えます。"
    services="machine-learning"
    documentationCenter=""
    authors="bradsev"
    manager="paulettm"
    editor="cgronlun" />

<tags
    ms.service="machine-learning"
    ms.workload="data-services"
    ms.tgt_pltfrm="na"
    ms.devlang="na"
    ms.topic="article"
    ms.date="10/20/2015"
    ms.author="bradsev" />


# 機械学習を強化するためのデータを準備するタスク

## はじめに
データの前処理とクリーニングは、通常は、機械学習でデータセットを効果的に使用する前に行う必要がある重要なタスクです。 未加工のデータは、多くの場合、ノイズが多く、信頼性が低く、値が欠落している可能性もあります。 このようなデータを使用してモデリングを行うと、誤解を招く結果が生成されることがあります。 これらのタスクは、Cortana Analytics Process (CAP) の一部です。通常、必要な前処理の検出と計画に使用されるデータセットの初期の探察の後に行われます。 詳細については、CAP プロセスについてで説明する手順を参照してください、 [Cortana Analytics Process](cortana-analytics-process.md)します。

前処理とクリーニングのタスクは、データ探索タスクと同様に、データの格納場所と形式に応じて、多様な環境 (SQL、Hive、Azure Machine Learning Studio など)、多様なツール、多様な言語 (R、Python など) で実行できます。 キャップが反復的な性質のため、これらのタスクは、プロセスのワークフローのさまざまな手順で行うことができます。

この記事では、多様なデータ処理の概念と、データを Azure Machine Learning に取り込む前または後に実行できるさまざまなタスクを紹介します。 

データの探索と Azure Machine Learning studio 内で実行を事前処理の例は、次を参照してください。、 [Azure ML Studio でのデータを前処理](http://azure.microsoft.com/documentation/videos/preprocessing-data-in-azure-ml-studio/) ビデオです。


## なぜデータの前処理とクリーニングを行うのか?

実世界のデータはさまざまなソースやプロセスから収集されるため、不規則なデータや破損されたデータが含まれていて、データセットの品質が損なわれることがあります。 よく発生するデータ品質の問題には、次のようなものがあります。

* **不完全な**: 属性または欠損値を含むデータが不足しています。
* **ノイズの多い**: データに誤りがあるレコードや外れ値が含まれています。
* **一貫性のない**: データには、競合するレコードや食い違いが含まれています。

質の良いデータは、高品質の予測モデルの前提です。 "ゴミを入れればゴミが出てくる" (ナンセンスなデータからはナンセンスな結果しか出てこない) を回避し、データの品質、ひいてはモデルのパフォーマンスを向上させるには、データの正常性スクリーニングを実施してデータの問題点を早期に発見し、それに対処するための前処理とクリーニングの手順を決定する必要があります。

## よく実行されるデータの正常性スクリーニングには何があるか?

全般的なデータの品質を確認するには、次の事項をチェックします。

* 数 **レコード**します。
* 数 **属性** (または **機能**)。
* 属性 **データ型** (名義、順序、または継続的な)。
* 数 **欠損値**です。
* **整形** データ。 
    * データが TSV または CSV の場合は、列の区切り記号と行の区切り記号によって常に正しく列と行が分離されていることを確認します。 
    * データが HTML または XML 形式の場合は、それぞれの標準に基づいてデータが整形式かどうかを確認します。 
    * また、部分的に構造化されたデータやまったく構造化されていないデータから構造化された情報を抽出するためにも解析が必要になります。
* **一貫性のないデータ レコード**します。 値の範囲は許可を確認してください。 たとえば、データには、学生の GPA、GPA が、範囲が指定されているかどうか確認が含まれている場合は、たとえば 0 ~ 4 です。

データに関する問題が見つかったら **処理手順** 多くの場合は、欠損値、データの正規化、分離のクリーニングが必要なは、削除/置換するテキストの処理には、データの整列を与える可能性がある文字が埋め込まれている、データ型が混在共通のフィールド、およびその他のユーザーです。

**Azure Machine Learning は表形式のデータを整形**します。  データが既に表形式である場合は、データの前処理を ML Studio 内で Azure Machine Learning を使用して直接実行できます。  データが表形式でない場合は (たとえば、XML)、データを表形式に変換するための解析が必要になることがあります。  

## データの前処理の主要なタスクにはどのようなものがあるか?

* **データのクリーニング**: 入力、または欠損値が検出して、ノイズ データや外れ値を削除します。
* **データ変換**: データを正規化次元やノイズを少なきます。
* **データの削減**: データ レコードまたはデータを処理しやすく属性をサンプリングします。
* **データの離散化**: 機械学習の特定のメソッドと連続属性を使いやすいようにカテゴリ別の属性に変換します。
* **テキストのクリーニング**: レコードなどが中断する改行が、タブ区切りのデータ ファイル内の埋め込みのタブに埋め込まれているなど、データの不整合を引き起こす可能性のある埋め込みの文字を削除します。

以下のセクションで、いくつかのデータの前処理ステップについて説明します。

## 欠落値をどのように処理するか?

欠落値に対処するには、問題を扱いやすくするため、まず欠落値が発生している原因を識別することをお勧めします。 欠落値を処理するための一般的な方法には、次のようなものがあります。

* **削除**: 欠損値を含むレコードを削除します。
* **ダミーへの置き換え**: ダミーの値で欠損値を置き換えます: 例:、 _不明な_ カテゴリ、または 0 の数値。
* **という意味では置換**: 不足しているデータが数値の場合は、平均値を持つ、欠落値を置き換えます。
* **頻繁に使用する代替**: カテゴリのデータがない場合は、最も多い項目に、欠落値を置き換えます
* **回帰値で置き換える**: 回帰メソッドを使用して欠損値を回帰値に置き換えます。  

## データをどのように正規化するか?

データの正規化では、指定した範囲に数値をスケーリングしなおします。 よく使用されるデータの正規化方法は、次のとおりです。

* **最小最大正規化**:、たとえば 0 ~ 1 の最小値は 0 と最大値を 1 にスケーリングする場所の間に線形範囲にデータを変換します。
* **Z スコア正規化**: 平均と標準偏差に基づいてデータをスケーリングします。 データと平均の差の標準偏差で除算します。
* **小数点スケーリング**: 属性値の小数点を移動して、データをスケーリングします。  

## データをどのように離散化するか?

データを離散化するには、連続値を名義属性または区間に変換します。 たとえば、次のような方法があります。

* **等幅ビン分割**: 属性のすべての可能な値の範囲を同じサイズの N のグループに分割し、ビン番号、箱にある値を割り当てます。
* **同じ高さのビニング**: のインスタンスを同じ個数ずつ n 個のグループに属性のすべての可能な値の範囲を分割し、ビン番号ビンに分類される、値を割り当てます。  

## データをどのように縮小するか? 

データを処理しやすくするためにデータのサイズを縮小するには、さまざまな方法があります。 データのサイズと領域に応じて、次の方法を適用できます。

* **レコードのサンプリング**: データ レコードをサンプリングし、データの代表的なサブセットのみを選択します。
* **属性のサンプリング**: データから最も重要な属性のサブセットのみを選択します。  
* **集計**: データをグループに分割し、各グループの数値を保存します。 たとえば、レストラン チェーンの過去 20 年間の毎日の売上の数値を毎月の売上に集計することにより、データのサイズを小さくします。  

## テキスト データをどのようにクリーニングするか?

**表形式データ内のテキスト フィールド** 列の配置やレコードの境界に影響する文字を含めることができます。 たとえば、タブ区切りファイルの項目に埋め込みタブが含まれていると、列の不整合が発生します。また、改行文字が含まれているとレコードの行が中断されます。 テキストの読み取り/書き込み時にテキスト エンコードの処理を誤ると、情報の損失や読み取り不能な文字 (null など) の間違った挿入が発生し、テキスト解析に影響が出る可能性があります。 テキスト フィールドが適切に配置されるようにクリーニングし、構造化されていない、または部分的に構造化されたテキスト データから構造化されたデータを抽出するには、注意深い解析と編集が必要になることがあります。

**データの探索** データの初期ビューが提供されます。 データの問題点が明らかになりこの手順では、この問題に対応するメソッドを適用できます。  その場合は、問題の原因は何か、その問題点はどのようにして発生したのかと考えながら対処することが重要です。 そのように作業を進めていくことは、問題点を解決するのに必要なデータ処理手順を決定するためにも役立ちます。 また、データから引き出そうとしている洞察の種類によっても、データの処理作業の優先順位が変わります。

## 参照

>*Data Mining: Concepts and Techniques*, 、第 3 版、Morgan Kaufmann、2011 年、Jiawei Han、Micheline Kamber、Jian pei 共著
 
